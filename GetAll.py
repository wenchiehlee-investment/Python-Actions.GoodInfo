#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
FIXED GetAll.py with Corrected 24-Hour Freshness Policy (v1.8.2 FIXED)
FIXES: Timestamp inconsistency, proper freshness validation, accurate success tracking
"""

import sys
import csv
import subprocess
import os
import time
import pandas as pd
import signal
from datetime import datetime, timedelta
import psutil
import shutil

# Try to set UTF-8 encoding for Windows console
try:
    if sys.platform.startswith('win'):
        import locale
        os.system('chcp 65001 > nul 2>&1')
except:
    pass

# Data type descriptions (unchanged)
DATA_TYPE_DESCRIPTIONS = {
    '1': 'Dividend Policy (è‚¡åˆ©æ”¿ç­–) - Weekly automation (Monday 8 AM UTC)',
    '2': 'Basic Info (åŸºæœ¬è³‡æ–™) - Manual only',
    '3': 'Stock Details (å€‹è‚¡å¸‚æ³) - Manual only',
    '4': 'Business Performance (ç¶“ç‡Ÿç¸¾æ•ˆ) - Weekly automation (Tuesday 8 AM UTC)',
    '5': 'Monthly Revenue (æ¯æœˆç‡Ÿæ”¶) - Daily automation (12 PM UTC)',
    '6': 'Equity Distribution (è‚¡æ¬Šçµæ§‹) - Weekly automation (Wednesday 8 AM UTC)',
    '7': 'Quarterly Performance (æ¯å­£ç¶“ç‡Ÿç¸¾æ•ˆ) - Weekly automation (Thursday 8 AM UTC)',
    '8': 'EPS x PER Weekly (æ¯é€±EPSæœ¬ç›Šæ¯”) - Weekly automation (Friday 8 AM UTC)',
    '9': 'Quarterly Analysis (å„å­£è©³ç´°çµ±è¨ˆè³‡æ–™) - Weekly automation (Saturday 8 AM UTC)',
    '10': 'Equity Class Weekly (è‚¡æ±æŒè‚¡åˆ†ç´šé€±) - Weekly automation (Sunday 8 AM UTC)'
}

# Global variables for graceful termination
current_results_data = {}
current_process_times = {}
current_retry_stats = {}
current_stock_ids = []
current_parameter = ""
current_stock_mapping = {}

def signal_handler(signum, frame):
    """Handle termination signals gracefully - save CSV before exit"""
    print(f"\nè­¦å‘Š æ”¶åˆ°çµ‚æ­¢ä¿¡è™Ÿ ({signum}) - æ­£åœ¨å„²å­˜é€²åº¦...")
    
    if current_results_data and current_stock_ids:
        try:
            save_csv_results_fixed(current_parameter, current_stock_ids, 
                                   current_results_data, current_process_times, 
                                   current_stock_mapping, current_retry_stats)
            processed_count = len(current_results_data)
            success_count = sum(1 for success in current_results_data.values() if success)
            total_attempts = sum(stats.get('attempts', 1) for stats in current_retry_stats.values()) if current_retry_stats else processed_count
            print(f"ç·Šæ€¥å„²å­˜å®Œæˆ: {processed_count} è‚¡ç¥¨å·²è™•ç†ï¼Œ{success_count} æˆåŠŸï¼Œ{total_attempts} ç¸½å˜—è©¦æ¬¡æ•¸")
        except Exception as e:
            print(f"ç·Šæ€¥å„²å­˜å¤±æ•—: {e}")
    
    print("ç¨‹å¼å·²å®‰å…¨çµ‚æ­¢")
    sys.exit(0)

# Register signal handlers
signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)

def aggressive_chrome_cleanup():
    """Enhanced Chrome process and resource cleanup"""
    cleanup_count = 0
    temp_cleanup_count = 0
    
    try:
        print("ğŸ”¥ åŸ·è¡Œå¼·åŒ– Chrome æ¸…ç†...")
        
        # Method 1: Kill Chrome processes using psutil
        for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
            try:
                if proc.info['name'] and any(name in proc.info['name'].lower() 
                                           for name in ['chrome', 'chromium', 'chromedriver']):
                    proc.terminate()
                    cleanup_count += 1
                    time.sleep(0.1)
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                pass
        
        # Wait for graceful termination
        time.sleep(2)
        
        # Method 2: Force kill remaining processes
        for proc in psutil.process_iter(['pid', 'name']):
            try:
                if proc.info['name'] and any(name in proc.info['name'].lower() 
                                           for name in ['chrome', 'chromium']):
                    proc.kill()
                    cleanup_count += 1
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                pass
        
        # Method 3: Unix pkill as backup
        try:
            os.system('pkill -f chrome > /dev/null 2>&1')
            os.system('pkill -f chromium > /dev/null 2>&1') 
            os.system('pkill -f chromedriver > /dev/null 2>&1')
        except:
            pass
        
        # Clean temporary directories
        temp_patterns = ['chrome', 'chromium', 'goodinfo', 'selenium', 'webdriver']
        temp_dirs = ['/tmp', '/var/tmp', os.path.expanduser('~/.cache')]
        
        for temp_dir in temp_dirs:
            if os.path.exists(temp_dir):
                try:
                    for item in os.listdir(temp_dir):
                        if any(pattern in item.lower() for pattern in temp_patterns):
                            item_path = os.path.join(temp_dir, item)
                            try:
                                if os.path.isdir(item_path):
                                    shutil.rmtree(item_path, ignore_errors=True)
                                else:
                                    os.remove(item_path)
                                temp_cleanup_count += 1
                            except:
                                pass
                except:
                    pass
        
        print(f"   Chrome ç¨‹åºæ¸…ç†: {cleanup_count} å€‹ç¨‹åº")
        print(f"   æš«å­˜æª”æ¡ˆæ¸…ç†: {temp_cleanup_count} å€‹é …ç›®")
        return cleanup_count + temp_cleanup_count
        
    except Exception as e:
        print(f"æ¸…ç†éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        return 0

def get_file_age_hours(file_path):
    """FIXED: Get accurate file age in hours"""
    try:
        if not os.path.exists(file_path):
            return float('inf')  # File doesn't exist, infinitely old
        
        file_mtime = os.path.getmtime(file_path)
        file_datetime = datetime.fromtimestamp(file_mtime)
        now = datetime.now()
        time_diff = now - file_datetime
        hours_ago = time_diff.total_seconds() / 3600
        
        return hours_ago
    except Exception as e:
        print(f"éŒ¯èª¤ Error calculating file age for {file_path}: {e}")
        return float('inf')

def validate_download_success(file_path, start_time, min_size_bytes=1024):
    """FIXED: Validate that download actually succeeded with proper file"""
    try:
        if not os.path.exists(file_path):
            return False, "File does not exist"
        
        # Check file size
        file_size = os.path.getsize(file_path)
        if file_size < min_size_bytes:
            return False, f"File too small ({file_size} bytes)"
        
        # Check if file was modified after we started
        file_mtime = os.path.getmtime(file_path)
        if file_mtime < start_time:
            return False, f"File timestamp ({datetime.fromtimestamp(file_mtime)}) predates process start ({datetime.fromtimestamp(start_time)})"
        
        # Check if it's actually an XLS/Excel file
        with open(file_path, 'rb') as f:
            header = f.read(8)
            # Basic check for Excel file signatures
            if not (header.startswith(b'\xd0\xcf\x11\xe0') or  # OLE compound document
                   header.startswith(b'PK') or  # ZIP-based (xlsx)
                   b'<html' in header.lower()):  # Could be HTML formatted as XLS
                return False, "File doesn't appear to be a valid Excel file"
        
        return True, "File validation passed"
        
    except Exception as e:
        return False, f"Validation error: {str(e)}"

def run_get_good_info_with_retry_fixed(stock_id, parameter, debug_mode=False, max_retries=3):
    """
    FIXED: Enhanced GetGoodInfo.py execution with proper success validation
    """
    timeout_config = {
        '1': 90,   '2': 60,   '3': 60,   '4': 75,   '5': 90,
        '6': 90,   '7': 90,   '8': 90,   '9': 75,   '10': 90
    }
    
    base_timeout = timeout_config.get(str(parameter), 75)
    backoff_delays = [0, 10, 30, 60]
    
    # Determine expected file path for validation
    folder_mapping = {
        '1': 'DividendDetail', '2': 'BasicInfo', '3': 'StockDetail',
        '4': 'StockBzPerformance', '5': 'ShowSaleMonChart', '6': 'EquityDistribution',
        '7': 'StockBzPerformance1', '8': 'ShowK_ChartFlow', '9': 'StockHisAnaQuar',
        '10': 'EquityDistributionClassHis'
    }
    folder = folder_mapping.get(parameter, f'DataType{parameter}')
    
    # Get company name for filename
    company_name = current_stock_mapping.get(stock_id, f'è‚¡ç¥¨{stock_id}')
    if parameter == '7':
        expected_filename = f"StockBzPerformance1_{stock_id}_{company_name}_quarter.xls"
    else:
        expected_filename = f"{folder}_{stock_id}_{company_name}.xls"
    
    expected_file_path = os.path.join(folder, expected_filename)
    
    start_time = time.time()
    process_start_timestamp = start_time  # For file validation
    last_error = ""
    
    print(f"   æœŸæœ›æª”æ¡ˆè·¯å¾‘: {expected_file_path}")
    
    for attempt in range(1, max_retries + 2):
        try:
            # Resource cleanup before retry attempts
            if attempt > 1:
                print(f"   ç¬¬ {attempt} æ¬¡å˜—è©¦ - åŸ·è¡Œè³‡æºæ¸…ç†...")
                cleanup_count = aggressive_chrome_cleanup()
                
                # Progressive backoff delay
                delay = backoff_delays[min(attempt - 1, len(backoff_delays) - 1)]
                if delay > 0:
                    print(f"   ç­‰å¾… {delay} ç§’å†·å»æ™‚é–“...")
                    time.sleep(delay)
            
            current_timeout = base_timeout + (attempt - 1) * 30
            
            print(f"   å˜—è©¦ {attempt}/4 (è¶…æ™‚: {current_timeout}s)")
            
            # Record pre-execution file state
            pre_execution_exists = os.path.exists(expected_file_path)
            pre_execution_mtime = None
            if pre_execution_exists:
                pre_execution_mtime = os.path.getmtime(expected_file_path)
                print(f"   åŸ·è¡Œå‰æª”æ¡ˆç‹€æ…‹: å­˜åœ¨, ä¿®æ”¹æ™‚é–“ {datetime.fromtimestamp(pre_execution_mtime)}")
            else:
                print(f"   åŸ·è¡Œå‰æª”æ¡ˆç‹€æ…‹: ä¸å­˜åœ¨")
            
            # Prepare command
            cmd = ['python', 'GetGoodInfo.py', str(stock_id), str(parameter)]
            
            # Set environment
            env = os.environ.copy()
            env['PYTHONIOENCODING'] = 'utf-8'
            
            # Execute with timeout
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=current_timeout,
                env=env,
                encoding='utf-8',
                errors='replace'
            )
            
            # FIXED: Proper success validation
            if result.returncode == 0:
                # Validate the download actually worked
                success_valid, validation_msg = validate_download_success(
                    expected_file_path, process_start_timestamp
                )
                
                if success_valid:
                    duration = time.time() - start_time
                    success_msg = f"âœ… {stock_id} ç¬¬ {attempt} æ¬¡å˜—è©¦æˆåŠŸ (å·²é©—è­‰æª”æ¡ˆ)"
                    if attempt > 1:
                        success_msg += f" (å‰ {attempt-1} æ¬¡å¤±æ•—å¾Œé‡è©¦æˆåŠŸ)"
                    print(success_msg)
                    print(f"   æª”æ¡ˆé©—è­‰: {validation_msg}")
                    
                    # Show output for retries or debug mode
                    if (debug_mode or attempt > 1) and result.stdout:
                        output_lines = result.stdout.strip().split('\n')
                        if len(output_lines) <= 3:
                            print(f"   è¼¸å‡º: {result.stdout.strip()}")
                        else:
                            print(f"   è¼¸å‡º: {output_lines[0]}")
                            print(f"        ... ({len(output_lines)} è¡Œè¼¸å‡º)")
                    
                    return True, attempt, "", duration
                else:
                    # Process returned 0 but file validation failed
                    error_msg = f"æª”æ¡ˆé©—è­‰å¤±æ•—: {validation_msg}"
                    last_error = error_msg
                    print(f"   âŒ ç¬¬ {attempt} æ¬¡å˜—è©¦å‡æˆåŠŸ: {error_msg}")
                    continue
            
            # Handle failure
            else:
                error_msg = f"é€€å‡ºç¢¼ {result.returncode}"
                if result.stderr and result.stderr.strip():
                    stderr_lines = result.stderr.strip().split('\n')
                    error_msg += f" - {stderr_lines[0]}"
                    if len(stderr_lines) > 1:
                        error_msg += f" (+{len(stderr_lines)-1} è¡ŒéŒ¯èª¤)"
                
                last_error = error_msg
                print(f"   âŒ ç¬¬ {attempt} æ¬¡å˜—è©¦å¤±æ•—: {error_msg}")
                
                # Don't retry certain error types
                if result.returncode in [2, 127]:
                    print(f"   ğŸ›‘ è‡´å‘½éŒ¯èª¤ï¼Œåœæ­¢é‡è©¦")
                    break
                
                continue
                
        except subprocess.TimeoutExpired:
            timeout_msg = f"è¶…æ™‚ ({current_timeout}ç§’)"
            last_error = timeout_msg
            print(f"   â° ç¬¬ {attempt} æ¬¡å˜—è©¦è¶…æ™‚: {timeout_msg}")
            
            # Force cleanup after timeout
            if attempt < max_retries + 1:
                print(f"   ğŸ§¹ è¶…æ™‚å¾ŒåŸ·è¡Œç·Šæ€¥æ¸…ç†...")
                aggressive_chrome_cleanup()
            
            continue
            
        except KeyboardInterrupt:
            print(f"   âš ï¸ ç”¨æˆ¶ä¸­æ–·åŸ·è¡Œ")
            raise
            
        except Exception as e:
            error_msg = f"åŸ·è¡Œç•°å¸¸: {str(e)}"
            last_error = error_msg
            print(f"   ğŸ’¥ ç¬¬ {attempt} æ¬¡å˜—è©¦ç•°å¸¸: {error_msg}")
            continue
    
    # All attempts failed
    duration = time.time() - start_time
    total_attempts = max_retries + 1
    print(f"   âŒ æœ€çµ‚å¤±æ•—: ç¶“é 4 æ¬¡å˜—è©¦ä»å¤±æ•—")
    print(f"   ğŸ“ æœ€å¾ŒéŒ¯èª¤: {last_error}")
    return False, total_attempts, last_error, duration

def determine_stocks_to_process_fixed(parameter, all_stock_ids, stock_mapping, debug_mode=False):
    """FIXED: Determine which stocks need processing with accurate file age calculation"""
    
    folder_mapping = {
        '1': 'DividendDetail', '2': 'BasicInfo', '3': 'StockDetail',
        '4': 'StockBzPerformance', '5': 'ShowSaleMonChart', '6': 'EquityDistribution',
        '7': 'StockBzPerformance1', '8': 'ShowK_ChartFlow', '9': 'StockHisAnaQuar',
        '10': 'EquityDistributionClassHis'
    }
    folder = folder_mapping.get(parameter, f'DataType{parameter}')
    
    # Load existing CSV data
    existing_data = {}
    csv_filepath = os.path.join(folder, "download_results.csv")
    
    if os.path.exists(csv_filepath):
        try:
            with open(csv_filepath, 'r', newline='', encoding='utf-8') as csvfile:
                reader = csv.DictReader(csvfile)
                for row in reader:
                    filename = row.get('filename', '')
                    if filename:
                        existing_data[filename] = {
                            'last_update_time': row.get('last_update_time', 'NEVER'),
                            'success': row.get('success', 'false'),
                            'process_time': row.get('process_time', 'NOT_PROCESSED'),
                            'retry_count': int(row.get('retry_count', 0))
                        }
        except Exception as e:
            print(f"ç„¡æ³•è®€å–ç¾æœ‰CSVæ•¸æ“š: {e}")
    
    # FIXED: Analyze status with proper file-based age calculation
    now = datetime.now()
    failed_stocks = []
    not_processed_stocks = []
    fresh_success = []
    expired_success = []
    
    print(f"ğŸ” åˆ†æ {len(all_stock_ids)} æ”¯è‚¡ç¥¨çš„æª”æ¡ˆç‹€æ…‹ (24å°æ™‚æ–°é®®åº¦æ”¿ç­–)...")
    
    for stock_id in all_stock_ids:
        company_name = stock_mapping.get(stock_id, f'è‚¡ç¥¨{stock_id}')
        
        # Generate expected filename and path
        if parameter == '7':
            filename = f"StockBzPerformance1_{stock_id}_{company_name}_quarter.xls"
        else:
            filename = f"{folder}_{stock_id}_{company_name}.xls"
        
        file_path = os.path.join(folder, filename)
        
        # FIXED: Check actual file existence and age
        file_exists = os.path.exists(file_path)
        
        if not file_exists:
            not_processed_stocks.append(stock_id)
            if debug_mode:
                print(f"   {stock_id}: æª”æ¡ˆä¸å­˜åœ¨ -> éœ€è™•ç†")
            continue
        
        # File exists - check its actual age
        file_age_hours = get_file_age_hours(file_path)
        
        # Check CSV record for additional context
        csv_record_exists = filename in existing_data
        csv_success = False
        if csv_record_exists:
            csv_success = existing_data[filename]['success'].lower() == 'true'
        
        # Decision logic based on file age (primary) and CSV record (secondary)
        if file_age_hours <= 24:
            fresh_success.append(stock_id)
            if debug_mode:
                print(f"   {stock_id}: æª”æ¡ˆ {file_age_hours:.1f}h æ–°é®® -> è·³é")
        elif file_age_hours > 24:
            if csv_success:
                expired_success.append(stock_id)
                if debug_mode:
                    print(f"   {stock_id}: æª”æ¡ˆ {file_age_hours:.1f}h éæœŸ -> éœ€æ›´æ–°")
            else:
                failed_stocks.append(stock_id)
                if debug_mode:
                    print(f"   {stock_id}: æª”æ¡ˆ {file_age_hours:.1f}h éæœŸä¸”æ›¾å¤±æ•— -> éœ€é‡è©¦")
        else:
            # Fallback for edge cases
            not_processed_stocks.append(stock_id)
            if debug_mode:
                print(f"   {stock_id}: ç„¡æ³•åˆ¤æ–·ç‹€æ…‹ -> éœ€è™•ç†")
    
    priority_stocks = failed_stocks + not_processed_stocks + expired_success
    
    print(f"è™•ç†ç‹€æ…‹åˆ†æ ({folder}) - FIXED 24å°æ™‚æ–°é®®åº¦æ”¿ç­–:")
    print(f"   å¤±æ•—è‚¡ç¥¨: {len(failed_stocks)} (æª”æ¡ˆå­˜åœ¨ä½†æ›¾æ¨™è¨˜å¤±æ•—)")
    print(f"   æœªè™•ç†è‚¡ç¥¨: {len(not_processed_stocks)} (æª”æ¡ˆä¸å­˜åœ¨)")
    print(f"   æ–°é®®æˆåŠŸ (â‰¤24å°æ™‚): {len(fresh_success)} (è·³é)")
    print(f"   éæœŸæˆåŠŸ (>24å°æ™‚): {len(expired_success)} (éœ€æ›´æ–°)")
    
    if priority_stocks:
        reprocess_reasons = []
        if failed_stocks:
            reprocess_reasons.append(f"{len(failed_stocks)}å€‹å¤±æ•—")
        if not_processed_stocks:
            reprocess_reasons.append(f"{len(not_processed_stocks)}å€‹æœªè™•ç†")
        if expired_success:
            reprocess_reasons.append(f"{len(expired_success)}å€‹éæœŸæˆåŠŸ")
        
        reason_str = "ã€".join(reprocess_reasons)
        print(f"éœ€è¦è™•ç†ç­–ç•¥: è™•ç† {len(priority_stocks)} å€‹è‚¡ç¥¨ ({reason_str})")
        return priority_stocks, "REPROCESS_NEEDED"
    elif fresh_success:
        print(f"ç„¡éœ€è™•ç†: æ‰€æœ‰ {len(fresh_success)} å€‹è‚¡ç¥¨åœ¨24å°æ™‚å…§å·²æˆåŠŸè™•ç†")
        return [], "UP_TO_DATE"
    else:
        print(f"åˆå§‹æƒæ: åŸ·è¡Œé¦–æ¬¡å®Œæ•´æƒæ")
        return all_stock_ids, "INITIAL_SCAN"

def save_csv_results_fixed(parameter, stock_ids, results_data, process_times, stock_mapping, retry_stats=None):
    """FIXED: Save CSV with accurate timestamps and file validation"""
    
    folder_mapping = {
        '1': 'DividendDetail', '2': 'BasicInfo', '3': 'StockDetail',
        '4': 'StockBzPerformance', '5': 'ShowSaleMonChart', '6': 'EquityDistribution',
        '7': 'StockBzPerformance1', '8': 'ShowK_ChartFlow', '9': 'StockHisAnaQuar',
        '10': 'EquityDistributionClassHis'
    }
    folder = folder_mapping.get(parameter, f'DataType{parameter}')
    
    if not os.path.exists(folder):
        os.makedirs(folder)
        print(f"å»ºç«‹è³‡æ–™å¤¾: {folder}")
    
    csv_filepath = os.path.join(folder, "download_results.csv")
    
    # Load existing data for stocks not processed in current run
    existing_data = {}
    if os.path.exists(csv_filepath):
        try:
            with open(csv_filepath, 'r', newline='', encoding='utf-8') as csvfile:
                reader = csv.DictReader(csvfile)
                for row in reader:
                    filename = row.get('filename', '')
                    if filename:
                        existing_data[filename] = {
                            'last_update_time': row.get('last_update_time', 'NEVER'),
                            'success': row.get('success', 'false'),
                            'process_time': row.get('process_time', 'NOT_PROCESSED'),
                            'retry_count': int(row.get('retry_count', 0))
                        }
        except Exception as e:
            print(f"è­¦å‘Š: ç„¡æ³•è¼‰å…¥ç¾æœ‰ CSV: {e}")
    
    try:
        with open(csv_filepath, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(['filename', 'last_update_time', 'success', 'process_time', 'retry_count'])
            
            for stock_id in stock_ids:
                company_name = stock_mapping.get(stock_id, f'è‚¡ç¥¨{stock_id}')
                
                if parameter == '7':
                    filename = f"StockBzPerformance1_{stock_id}_{company_name}_quarter.xls"
                else:
                    filename = f"{folder}_{stock_id}_{company_name}.xls"
                
                file_path = os.path.join(folder, filename)
                
                if stock_id in results_data:
                    # Current processing data - FIXED timestamp logic
                    success = str(results_data[stock_id]).lower()
                    process_time = process_times.get(stock_id, 'NOT_PROCESSED')
                    total_attempts = retry_stats.get(stock_id, {}).get('attempts', 1) if retry_stats else 1
                    retry_count = max(0, total_attempts - 1)
                    
                    # FIXED: Use ACTUAL file timestamp for last_update_time
                    if success == 'true' and os.path.exists(file_path):
                        try:
                            file_mtime = os.path.getmtime(file_path)
                            last_update = datetime.fromtimestamp(file_mtime).strftime('%Y-%m-%d %H:%M:%S')
                        except Exception as e:
                            print(f"è­¦å‘Š: ç„¡æ³•è®€å– {filename} çš„æª”æ¡ˆæ™‚é–“: {e}")
                            last_update = process_time  # Fallback to process time
                    else:
                        # Failed or file doesn't exist
                        if filename in existing_data:
                            last_update = existing_data[filename]['last_update_time']
                        else:
                            last_update = 'NEVER'
                else:
                    # Existing data (not processed in current run)
                    if filename in existing_data:
                        existing_record = existing_data[filename]
                        last_update = existing_record['last_update_time']
                        success = existing_record['success']
                        process_time = existing_record['process_time']
                        retry_count = existing_record.get('retry_count', 0)
                    else:
                        # New stock not yet processed
                        last_update = 'NEVER'
                        success = 'false'
                        process_time = 'NOT_PROCESSED'
                        retry_count = 0
                
                writer.writerow([filename, last_update, success, process_time, retry_count])
        
        print(f"FIXED CSVçµæœå·²å„²å­˜: {csv_filepath}")
        
        # Enhanced summary with file validation
        if results_data:
            total_stocks = len(stock_ids)
            processed_count = len(results_data)
            success_count = sum(1 for success in results_data.values() if success)
            success_rate = (success_count / processed_count * 100) if processed_count > 0 else 0
            
            # Validate actual files exist for successful entries
            actual_success_count = 0
            for stock_id in results_data:
                if results_data[stock_id]:  # Marked as successful in results
                    company_name = stock_mapping.get(stock_id, f'è‚¡ç¥¨{stock_id}')
                    if parameter == '7':
                        filename = f"StockBzPerformance1_{stock_id}_{company_name}_quarter.xls"
                    else:
                        filename = f"{folder}_{stock_id}_{company_name}.xls"
                    file_path = os.path.join(folder, filename)
                    
                    if os.path.exists(file_path):
                        file_age_hours = get_file_age_hours(file_path)
                        if file_age_hours <= 24:  # Recently updated
                            actual_success_count += 1
            
            print(f"{folder} æ‘˜è¦ (FIXEDç‰ˆæœ¬):")
            print(f"   CSV ç¸½è‚¡ç¥¨æ•¸: {total_stocks}")
            print(f"   æœ¬æ¬¡è™•ç†è‚¡ç¥¨æ•¸: {processed_count}")
            print(f"   æœ¬æ¬¡æ¨™è¨˜æˆåŠŸ: {success_count}")
            print(f"   å¯¦éš›æª”æ¡ˆæˆåŠŸ: {actual_success_count}")
            print(f"   æœ¬æ¬¡æˆåŠŸç‡: {success_rate:.1f}%")
            print(f"   æª”æ¡ˆé©—è­‰ç‡: {(actual_success_count/success_count*100) if success_count > 0 else 0:.1f}%")
            
            if retry_stats:
                total_attempts = sum(stats.get('attempts', 1) for stats in retry_stats.values())
                total_retries = sum(max(0, stats.get('attempts', 1) - 1) for stats in retry_stats.values())
                print(f"   ç¸½å˜—è©¦æ¬¡æ•¸: {total_attempts}")
                print(f"   ç¸½é‡è©¦æ¬¡æ•¸: {total_retries}")
            
            print(f"   CSV ä½ç½®: {csv_filepath}")
        
    except Exception as e:
        print(f"å„²å­˜ CSV æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

# Original helper functions (unchanged)
def read_stock_ids(csv_file):
    """Read stock IDs from CSV file with enhanced encoding support"""
    stock_ids = []
    encodings = ['utf-8', 'big5', 'utf-8-sig']
    
    for encoding in encodings:
        try:
            with open(csv_file, 'r', encoding=encoding) as f:
                first_line = f.readline()
                f.seek(0)
                reader = csv.reader(f)
                
                # Skip header if detected
                first_row = next(reader)
                if any('è‚¡' in str(cell) or 'StockID' in str(cell) or 'ID' in str(cell) 
                      or 'ä»£è™Ÿ' in str(cell) or 'Code' in str(cell) for cell in first_row):
                    print(f"åµæ¸¬åˆ°æ¨™é¡Œè¡Œ: {first_row}")
                else:
                    stock_id = first_row[0].strip() if first_row and len(first_row) > 0 else ""
                    if stock_id and stock_id.isdigit() and 4 <= len(stock_id) <= 6:
                        stock_ids.append(stock_id)
                
                # Read remaining rows
                for row in reader:
                    if row and len(row) > 0 and row[0].strip():
                        stock_id = row[0].strip()
                        if stock_id.isdigit() and 4 <= len(stock_id) <= 6:
                            stock_ids.append(stock_id)
            
            print(f"æˆåŠŸä½¿ç”¨ {encoding} ç·¨ç¢¼è®€å–æª”æ¡ˆ")
            break
            
        except UnicodeDecodeError:
            continue
        except Exception as e:
            print(f"ä½¿ç”¨ {encoding} ç·¨ç¢¼æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            continue
    
    return stock_ids

def load_stock_mapping(csv_file):
    """Load stock ID to company name mapping from CSV file"""
    stock_mapping = {}
    try:
        encodings = ['utf-8', 'big5', 'utf-8-sig']
        
        for encoding in encodings:
            try:
                df = pd.read_csv(csv_file, encoding=encoding)
                
                stock_id_col = None
                company_name_col = None
                
                for col in df.columns:
                    if any(keyword in str(col) for keyword in ['ä»£è™Ÿ', 'ID', 'Code', 'Stock']):
                        stock_id_col = col
                    elif any(keyword in str(col) for keyword in ['åç¨±', 'Name', 'Company', 'å…¬å¸']):
                        company_name_col = col
                
                if stock_id_col is not None and company_name_col is not None:
                    for _, row in df.iterrows():
                        stock_id = str(row[stock_id_col]).strip()
                        company_name = str(row[company_name_col]).strip()
                        if stock_id and company_name and stock_id != 'nan' and company_name != 'nan':
                            stock_mapping[stock_id] = company_name
                    
                    print(f"è¼‰å…¥ {len(stock_mapping)} å€‹è‚¡ç¥¨åç¨±å°æ‡‰")
                    break
                
            except UnicodeDecodeError:
                continue
            except Exception:
                continue
        
        if not stock_mapping:
            print("ç„¡æ³•è¼‰å…¥è‚¡ç¥¨åç¨±å°æ‡‰ï¼Œå°‡ä½¿ç”¨é è¨­åç¨±")
        
    except Exception as e:
        print(f"è¼‰å…¥è‚¡ç¥¨åç¨±å°æ‡‰æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    return stock_mapping

def show_enhanced_usage():
    """Show enhanced usage information for v1.8.2 FIXED"""
    print("=" * 70)
    print("Enhanced Batch Stock Data Downloader (v1.8.2 FIXED)")
    print("Complete 10 Data Types with FIXED 24-Hour Freshness Policy")
    print("FIXED: Timestamp consistency, proper file validation, accurate tracking")
    print("=" * 70)
    print()
    print("FIXED FEATURES:")
    print("   âœ… FIXED 24-Hour Policy: Proper file age calculation")
    print("   âœ… FIXED Timestamps: CSV matches actual file modification time")
    print("   âœ… FIXED Validation: Verify downloads actually succeeded")
    print("   âœ… FIXED Tracking: Accurate success/failure reporting")
    print("   ğŸ”§ Enhanced Debug: Detailed process tracking")
    print()
    print("Data Types (Complete 10 Types - v1.8.2 FIXED):")
    for dt, desc in DATA_TYPE_DESCRIPTIONS.items():
        print(f"   {dt} = {desc}")
    print()
    print("Options:")
    print("   --test   = Process only first 3 stocks (testing)")
    print("   --debug  = Show detailed error messages and file tracking")
    print("   --direct = Simple execution mode (compatibility test)")
    print()
    print("FIXED Examples (v1.8.2 - With proper validation):")
    print("   python GetAll.py 1          # FIXED: accurate freshness policy")
    print("   python GetAll.py 6 --debug  # FIXED: with detailed file tracking")  
    print("   python GetAll.py 7 --test   # FIXED: test mode with validation")
    print()

def main():
    """FIXED main function with corrected 24-hour freshness policy (v1.8.2)"""
    global current_results_data, current_process_times, current_stock_ids, current_parameter, current_stock_mapping
    
    print("=" * 70)
    print("Enhanced Batch Stock Data Downloader (v1.8.2 FIXED)")
    print("Complete 10 Data Types with FIXED 24-Hour Freshness Policy")
    print("FIXED: Timestamp consistency + proper file validation")
    print("Eliminates false successes and ensures data accuracy")
    print("=" * 70)
    
    # Check command line arguments
    if len(sys.argv) < 2:
        show_enhanced_usage()
        print("Error: Please provide DATA_TYPE parameter")
        print("Examples:")
        print("   python GetAll.py 1      # FIXED dividend data processing")
        print("   python GetAll.py 6      # FIXED equity distribution")
        print("   python GetAll.py 7      # FIXED quarterly performance")
        sys.exit(1)
    
    parameter = sys.argv[1]
    test_mode = '--test' in sys.argv
    debug_mode = '--debug' in sys.argv
    direct_mode = '--direct' in sys.argv
    csv_file = "StockID_TWSE_TPEX.csv"
    
    # Validate data type
    if parameter not in DATA_TYPE_DESCRIPTIONS:
        print(f"Invalid data type: {parameter}")
        print("Valid data types:")
        for dt, desc in DATA_TYPE_DESCRIPTIONS.items():
            print(f" {dt} = {desc}")
        sys.exit(1)
    
    # Check files
    if not os.path.exists(csv_file):
        print(f"[ERROR] æ‰¾ä¸åˆ°æª”æ¡ˆ: {csv_file}")
        print("è«‹å…ˆåŸ·è¡Œ Getè§€å¯Ÿåå–®.py ä¸‹è¼‰è‚¡ç¥¨æ¸…å–®")
        sys.exit(1)
    
    if not os.path.exists("GetGoodInfo.py"):
        print("[ERROR] æ‰¾ä¸åˆ° GetGoodInfo.py")
        sys.exit(1)
    
    # Read stock IDs and mapping
    print(f"[è®€å–] è®€å–è‚¡ç¥¨æ¸…å–®: {csv_file}")
    stock_ids = read_stock_ids(csv_file)
    
    if not stock_ids:
        print("[ERROR] æœªæ‰¾åˆ°æœ‰æ•ˆçš„è‚¡ç¥¨ä»£ç¢¼")
        sys.exit(1)
    
    print(f"[è®€å–] è¼‰å…¥è‚¡ç¥¨åç¨±å°æ‡‰...")
    stock_mapping = load_stock_mapping(csv_file)
    
    # Set global variables for signal handler
    current_stock_ids = stock_ids
    current_parameter = parameter  
    current_stock_mapping = stock_mapping
    global current_retry_stats
    current_retry_stats = {}
    
    print(f"[çµ±è¨ˆ] æ‰¾åˆ° {len(stock_ids)} æ”¯è‚¡ç¥¨")
    print(f"å‰5æ”¯è‚¡ç¥¨: {stock_ids[:5]}")
    
    data_desc = DATA_TYPE_DESCRIPTIONS.get(parameter, f"Data Type {parameter}")
    
    if test_mode:
        stock_ids = stock_ids[:3]
        print(f"[æ¸¬è©¦æ¨¡å¼] åªè™•ç†å‰ {len(stock_ids)} æ”¯è‚¡ç¥¨")
    
    if debug_mode:
        print("[é™¤éŒ¯æ¨¡å¼] å°‡é¡¯ç¤ºè©³ç´°æª”æ¡ˆè¿½è¹¤å’Œé©—è­‰éç¨‹")
    
    print(f"è³‡æ–™é¡å‹: {data_desc}")
    print(f"åƒæ•¸: {parameter}")
    print(f"ğŸ”§ FIXEDè™•ç†: æº–ç¢ºçš„24å°æ™‚æ–°é®®åº¦æ”¿ç­–")
    print(f"âœ… FIXEDé©—è­‰: ç¢ºä¿ä¸‹è¼‰å¯¦éš›æˆåŠŸ")
    print(f"ğŸ“Š FIXEDè¿½è¹¤: CSVæ™‚æˆ³å°æ‡‰å¯¦éš›æª”æ¡ˆæ™‚é–“")
    
    print(f"é–‹å§‹æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("-" * 70)
    
    # FIXED smart processing analysis
    print("FIXED æ™ºæ…§è™•ç†åˆ†æä¸­ (ä¿®æ­£çš„24å°æ™‚æ–°é®®åº¦æ”¿ç­–)...")
    stocks_to_process, processing_strategy = determine_stocks_to_process_fixed(
        parameter, stock_ids, stock_mapping, debug_mode
    )
    
    if not stocks_to_process:
        print("æ‰€æœ‰è³‡æ–™éƒ½æ˜¯æ–°é®®çš„ (24å°æ™‚å…§)ï¼Œç„¡éœ€è™•ç†!")
        save_csv_results_fixed(parameter, stock_ids, {}, {}, stock_mapping, {})
        print("ä»»å‹™å®Œæˆ!")
        return
    
    # Update processing list for test mode
    original_count = len(stock_ids)
    if test_mode and processing_strategy != "UP_TO_DATE":
        stocks_to_process = stocks_to_process[:3]
        print(f"[æ¸¬è©¦æ¨¡å¼] é™åˆ¶è™•ç† {len(stocks_to_process)} æ”¯è‚¡ç¥¨")
    
    processing_count = len(stocks_to_process)
    print(f"è™•ç†ç­–ç•¥: {processing_strategy}")
    print(f"è™•ç†ç¯„åœ: {processing_count}/{original_count} æ”¯è‚¡ç¥¨")
    print(f"ğŸ”§ FIXED: æ¯æ”¯è‚¡ç¥¨æœ€å¤š 4 æ¬¡å˜—è©¦æ©Ÿæœƒ (1+3)")
    print(f"âœ… FIXED: åš´æ ¼æª”æ¡ˆé©—è­‰ç¢ºä¿çœŸå¯¦æˆåŠŸ")
    print("-" * 70)
    
    # Enhanced batch processing with FIXED retry mechanism
    success_count = 0
    failed_count = 0
    results_data = {}
    process_times = {}
    retry_stats = {}
    
    # Initialize CSV with FIXED logic
    print(f"åˆå§‹åŒ– FIXED CSV æª”æ¡ˆ...")
    save_csv_results_fixed(parameter, stock_ids, {}, {}, stock_mapping, {})
    
    # Process stocks with FIXED retry mechanism
    total_attempts = 0
    for i, stock_id in enumerate(stocks_to_process, 1):
        print(f"\n[{i}/{len(stocks_to_process)}] è™•ç†è‚¡ç¥¨: {stock_id}")
        
        # Record start time
        current_process_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        process_times[stock_id] = current_process_time
        
        # Execute with FIXED retry mechanism
        success, attempts, error_msg, duration = run_get_good_info_with_retry_fixed(
            stock_id, parameter, debug_mode, max_retries=3
        )
        
        # Record results
        results_data[stock_id] = success
        retry_stats[stock_id] = {
            'attempts': attempts,
            'error': error_msg,
            'duration': duration
        }
        total_attempts += attempts
        
        # Update global variables for signal handler
        current_results_data = results_data.copy()
        current_process_times = process_times.copy()
        current_retry_stats = retry_stats.copy()
        
        if success:
            success_count += 1
            if attempts > 1:
                print(f"   ğŸ¯ é‡è©¦æˆåŠŸ: ç¬¬ {attempts} æ¬¡å˜—è©¦æˆåŠŸ")
        else:
            failed_count += 1
            print(f"   ğŸ’¥ æœ€çµ‚å¤±æ•—: {attempts} æ¬¡å˜—è©¦å¾Œå¤±æ•— (æœ€å¤š4æ¬¡)")
        
        # Save progress after each stock with FIXED logic
        try:
            save_csv_results_fixed(parameter, stock_ids, results_data, process_times, stock_mapping, retry_stats)
            print(f"   ğŸ“ FIXED CSV å·²æ›´æ–° ({i}/{len(stocks_to_process)} å®Œæˆ)")
        except Exception as e:
            print(f"   âš ï¸ CSV æ›´æ–°å¤±æ•—: {e}")
        
        # Delay between stocks
        if i < len(stocks_to_process):
            delay = 3 if success else 5
            time.sleep(delay)
    
    # Final CSV save with FIXED logic
    print("\n" + "=" * 70)
    print("æœ€çµ‚ FIXED CSV çµæœ...")
    save_csv_results_fixed(parameter, stock_ids, results_data, process_times, stock_mapping, retry_stats)
    
    # Enhanced Summary with FIXED validation
    print("\n" + "=" * 70)
    print("Enhanced Execution Summary (v1.8.2 FIXED) - Corrected Processing")
    print("Complete 10 Data Types + FIXED Timestamp + Proper Validation")
    print("=" * 70)
    print(f"è³‡æ–™é¡å‹: {data_desc}")
    print(f"è™•ç†ç­–ç•¥: {processing_strategy}")
    print(f"ç¸½è‚¡ç¥¨æ•¸: {original_count} æ”¯")
    print(f"éœ€è™•ç†è‚¡ç¥¨æ•¸: {processing_count} æ”¯") 
    print(f"å¯¦éš›è™•ç†: {len(stocks_to_process)} æ”¯è‚¡ç¥¨")
    print(f"æœ€çµ‚æˆåŠŸ: {success_count} æ”¯")
    print(f"æœ€çµ‚å¤±æ•—: {failed_count} æ”¯")
    print(f"ç¸½å˜—è©¦æ¬¡æ•¸: {total_attempts} æ¬¡")
    print(f"å¹³å‡å˜—è©¦æ¬¡æ•¸: {total_attempts/len(stocks_to_process):.1f} æ¬¡/è‚¡ç¥¨")
    
    if processing_count > 0:
        final_success_rate = (success_count / processing_count * 100)
        print(f"ğŸ¯ FIXED æœ€çµ‚æˆåŠŸç‡: {final_success_rate:.1f}% (å«åš´æ ¼é©—è­‰)")
    
    # Show FIXED improvements
    print(f"\nâœ… FIXED æ”¹å–„é …ç›®:")
    print(f"   â€¢ æ™‚æˆ³ä¸€è‡´æ€§: CSVè¨˜éŒ„ç¾åœ¨å°æ‡‰å¯¦éš›æª”æ¡ˆæ™‚é–“")
    print(f"   â€¢ æª”æ¡ˆé©—è­‰: ç¢ºä¿ä¸‹è¼‰å¯¦éš›æˆåŠŸä¸”æª”æ¡ˆæœ‰æ•ˆ")
    print(f"   â€¢ æ–°é®®åº¦æ”¿ç­–: æ­£ç¢ºè¨ˆç®—æª”æ¡ˆå¹´é½¡ï¼Œé¿å…èª¤åˆ¤")
    print(f"   â€¢ æˆåŠŸè¿½è¹¤: æ¶ˆé™¤å‡æˆåŠŸï¼Œæä¾›æº–ç¢ºç‹€æ…‹")
    
    print(f"\nçµæŸæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    if failed_count > 0:
        print(f"\nâš ï¸ ä»æœ‰ {failed_count} æ”¯è‚¡ç¥¨ç¶“4æ¬¡å˜—è©¦å¾Œå¤±æ•—")
        print("å»ºè­°:")
        print("   â€¢ æª¢æŸ¥ç¶²è·¯é€£ç·šç‹€æ…‹")
        print("   â€¢ ä½¿ç”¨ --debug æŸ¥çœ‹è©³ç´°éŒ¯èª¤")
        print("   â€¢ å–®ç¨åŸ·è¡Œå¤±æ•—è‚¡ç¥¨: python GetGoodInfo.py [è‚¡ç¥¨ä»£è™Ÿ] [é¡å‹]")
        print("   â€¢ FIXED: ç¾åœ¨èƒ½æº–ç¢ºå€åˆ†çœŸå¯¦å¤±æ•—å’Œå‡æˆåŠŸ")
    else:
        print(f"\nğŸ‰ å®Œç¾åŸ·è¡Œ! æ‰€æœ‰ {success_count} æ”¯è‚¡ç¥¨å‡è™•ç†æˆåŠŸ")
        if total_attempts > len(stocks_to_process):
            improvement = total_attempts - len(stocks_to_process)
            print(f"ğŸ’ª é‡è©¦æ©Ÿåˆ¶é¡å¤–æŒ½æ•‘äº† {improvement} æ¬¡å¤±æ•—")
        print(f"âœ… FIXEDç‰ˆæœ¬æä¾›æº–ç¢ºçš„æˆåŠŸé©—è­‰å’Œæ™‚æˆ³è¿½è¹¤")

if __name__ == "__main__":
    main()