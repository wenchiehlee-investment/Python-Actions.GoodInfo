#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GetAll.py - Enhanced Batch Processing for GoodInfo.tw Data (v1.6.0)
Reads stock IDs from StockID_TWSE_TPEX.csv and calls GetGoodInfo.py for each stock
Supports all 7 data types with intelligent processing and CSV success tracking
Version: v15 - Smart Processing Priority System

SMART PROCESSING FEATURES:
1. Priority Processing: Handles failed/unprocessed stocks first
2. Smart Refresh: Full scan only when all data is successful but old  
3. Skip Up-to-date: Avoids re-processing recent successful downloads
4. Graceful Termination: Never lose progress on cancellation

Usage: python GetAll.py <parameter> [options]
Examples: 
  python GetAll.py 1          # Dividend data with smart processing
  python GetAll.py 6 --test   # Equity distribution for first 3 stocks (NEW!)
  python GetAll.py 7 --debug  # Quarterly performance with debug output (NEW!)
"""

import sys
import csv
import subprocess
import os
import time
import pandas as pd
import signal
from datetime import datetime

# Try to set UTF-8 encoding for Windows console
try:
    if sys.platform.startswith('win'):
        import locale
        # Set console to handle UTF-8 if possible
        os.system('chcp 65001 > nul 2>&1')
except:
    pass

# Data type descriptions for v1.5.0
DATA_TYPE_DESCRIPTIONS = {
    '1': 'Dividend Policy (æ®–åˆ©ç‡æ”¿ç­–) - Daily automation',
    '2': 'Basic Info (åŸºæœ¬è³‡æ–™) - Manual only',
    '3': 'Stock Details (å€‹è‚¡å¸‚æ³) - Manual only',
    '4': 'Business Performance (ç¶“ç‡Ÿç¸¾æ•ˆ) - Daily automation',
    '5': 'Monthly Revenue (æ¯æœˆç‡Ÿæ”¶) - Daily automation',
    '6': 'Equity Distribution (è‚¡æ±çµæ§‹) - Daily automation (NEW!)',
    '7': 'Quarterly Performance (æ¯å­£ç¶“ç‡Ÿç¸¾æ•ˆ) - Daily automation (NEW!)'
}

# Global variables for graceful termination
current_results_data = {}
current_process_times = {}
current_stock_ids = []
current_parameter = ""
current_stock_mapping = {}

def signal_handler(signum, frame):
    """Handle termination signals gracefully - save CSV before exit"""
    print(f"\nğŸš¨ æ”¶åˆ°çµ‚æ­¢ä¿¡è™Ÿ ({signum}) - æ­£åœ¨å„²å­˜é€²åº¦...")
    
    if current_results_data and current_stock_ids:
        try:
            save_simple_csv_results(current_parameter, current_stock_ids, 
                                   current_results_data, current_process_times, 
                                   current_stock_mapping)
            processed_count = len(current_results_data)
            success_count = sum(1 for success in current_results_data.values() if success)
            print(f"âœ… ç·Šæ€¥å„²å­˜å®Œæˆ: {processed_count} è‚¡ç¥¨å·²è™•ç†ï¼Œ{success_count} æˆåŠŸ")
        except Exception as e:
            print(f"âŒ ç·Šæ€¥å„²å­˜å¤±æ•—: {e}")
    
    print("ğŸ‘‹ ç¨‹å¼å·²å®‰å…¨çµ‚æ­¢")
    sys.exit(0)

# Register signal handlers for graceful shutdown
signal.signal(signal.SIGINT, signal_handler)   # Ctrl+C
signal.signal(signal.SIGTERM, signal_handler)  # Termination signal

# Add this function after existing functions in GetAll.py
def load_existing_csv_data(folder_name):
    """Load existing CSV data from the specific folder"""
    csv_filepath = os.path.join(folder_name, "download_results.csv")
    existing_data = {}
    
    if os.path.exists(csv_filepath):
        try:
            with open(csv_filepath, 'r', newline='', encoding='utf-8') as csvfile:
                reader = csv.DictReader(csvfile)
                for row in reader:
                    filename = row.get('filename', '')
                    if filename:
                        existing_data[filename] = {
                            'last_update_time': row.get('last_update_time', 'NEVER'),
                            'success': row.get('success', 'false'),
                            'process_time': row.get('process_time', 'NOT_PROCESSED')
                        }
            print(f"ğŸ“– Loaded {len(existing_data)} existing records from {csv_filepath}")
        except Exception as e:
            print(f"âš ï¸ Warning: Could not load existing CSV: {e}")
    else:
        print(f"ğŸ“ No existing {csv_filepath} found - will create new file")
    
    return existing_data

def read_stock_ids(csv_file):
    """Read stock IDs from CSV file with enhanced encoding support"""
    stock_ids = []
    
    # Try different encodings
    encodings = ['utf-8', 'big5', 'utf-8-sig']
    
    for encoding in encodings:
        try:
            with open(csv_file, 'r', encoding=encoding) as f:
                # Try to detect if first line is header
                first_line = f.readline()
                f.seek(0)
                
                reader = csv.reader(f)
                
                # Skip header if it looks like one (contains Chinese characters or "StockID")
                first_row = next(reader)
                if any('è‚¡' in str(cell) or 'StockID' in str(cell) or 'ID' in str(cell) or 'ä»£è™Ÿ' in str(cell) or 'Code' in str(cell) for cell in first_row):
                    print(f"åµæ¸¬åˆ°æ¨™é ­è¡Œ: {first_row}")
                else:
                    # First row is data, add it back
                    stock_id = first_row[0].strip() if first_row and len(first_row) > 0 else ""
                    if stock_id and stock_id.isdigit() and 4 <= len(stock_id) <= 6:
                        stock_ids.append(stock_id)
                
                # Read remaining rows
                for row in reader:
                    if row and len(row) > 0 and row[0].strip():
                        # Assume stock ID is in first column
                        stock_id = row[0].strip()
                        # Basic validation - stock IDs are usually 4-6 digits
                        if stock_id.isdigit() and 4 <= len(stock_id) <= 6:
                            stock_ids.append(stock_id)
            
            print(f"æˆåŠŸä½¿ç”¨ {encoding} ç·¨ç¢¼è®€å–æª”æ¡ˆ")
            break
            
        except UnicodeDecodeError:
            continue
        except Exception as e:
            print(f"ä½¿ç”¨ {encoding} ç·¨ç¢¼æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            continue
    
    return stock_ids

def load_stock_mapping(csv_file):
    """Load stock ID to company name mapping from CSV file"""
    stock_mapping = {}
    try:
        encodings = ['utf-8', 'big5', 'utf-8-sig']
        
        for encoding in encodings:
            try:
                df = pd.read_csv(csv_file, encoding=encoding)
                
                # Check if required columns exist (try different possible column names)
                stock_id_col = None
                company_name_col = None
                
                for col in df.columns:
                    if any(keyword in str(col) for keyword in ['ä»£è™Ÿ', 'ID', 'Code', 'Stock']):
                        stock_id_col = col
                    elif any(keyword in str(col) for keyword in ['åç¨±', 'Name', 'Company', 'å…¬å¸']):
                        company_name_col = col
                
                if stock_id_col is not None and company_name_col is not None:
                    for _, row in df.iterrows():
                        stock_id = str(row[stock_id_col]).strip()
                        company_name = str(row[company_name_col]).strip()
                        if stock_id and company_name and stock_id != 'nan' and company_name != 'nan':
                            stock_mapping[stock_id] = company_name
                    
                    print(f"âœ… è¼‰å…¥ {len(stock_mapping)} å€‹è‚¡ç¥¨åç¨±å°æ‡‰")
                    break
                
            except UnicodeDecodeError:
                continue
            except Exception as e:
                continue
        
        if not stock_mapping:
            print("âš ï¸ ç„¡æ³•è¼‰å…¥è‚¡ç¥¨åç¨±å°æ‡‰ï¼Œå°‡ä½¿ç”¨é è¨­åç¨±")
        
    except Exception as e:
        print(f"âš ï¸ è¼‰å…¥è‚¡ç¥¨åç¨±å°æ‡‰æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    return stock_mapping

def determine_stocks_to_process(parameter, all_stock_ids, stock_mapping):
    """Determine which stocks need processing based on existing CSV data"""
    
    # Determine folder based on data type
    if parameter == '7':
        folder = "StockBzPerformance1"
    else:
        folder_mapping = {
            '1': 'DividendDetail',
            '2': 'BasicInfo', 
            '3': 'StockDetail',
            '4': 'StockBzPerformance',
            '5': 'ShowSaleMonChart',
            '6': 'EquityDistribution'
        }
        folder = folder_mapping.get(parameter, f'DataType{parameter}')
    
    # Load existing CSV data
    existing_data = {}
    csv_filepath = os.path.join(folder, "download_results.csv")
    
    if os.path.exists(csv_filepath):
        try:
            with open(csv_filepath, 'r', newline='', encoding='utf-8') as csvfile:
                reader = csv.DictReader(csvfile)
                for row in reader:
                    filename = row.get('filename', '')
                    if filename:
                        existing_data[filename] = {
                            'last_update_time': row.get('last_update_time', 'NEVER'),
                            'success': row.get('success', 'false'),
                            'process_time': row.get('process_time', 'NOT_PROCESSED')
                        }
        except Exception as e:
            print(f"âš ï¸ ç„¡æ³•è®€å–ç¾æœ‰CSVæ•¸æ“š: {e}")
    
    # Analyze current status
    today = datetime.now().strftime('%Y-%m-%d')
    failed_stocks = []
    not_processed_stocks = []
    successful_today = []
    successful_old = []
    
    for stock_id in all_stock_ids:
        company_name = stock_mapping.get(stock_id, f'è‚¡ç¥¨{stock_id}')
        
        # Generate expected filename
        if parameter == '7':
            filename = f"StockBzPerformance1_{stock_id}_{company_name}_quarter.xls"
        else:
            filename = f"{folder}_{stock_id}_{company_name}.xls"
        
        if filename in existing_data:
            record = existing_data[filename]
            success = record['success'].lower() == 'true'
            process_time = record['process_time']
            
            if process_time == 'NOT_PROCESSED':
                not_processed_stocks.append(stock_id)
            elif not success:  # success=false
                failed_stocks.append(stock_id)
            elif success:  # success=true
                if process_time.startswith(today):
                    successful_today.append(stock_id)
                else:
                    successful_old.append(stock_id)
        else:
            # No record exists
            not_processed_stocks.append(stock_id)
    
    # Decision logic
    priority_stocks = failed_stocks + not_processed_stocks
    
    print(f"ğŸ“Š è™•ç†ç‹€æ…‹åˆ†æ ({folder}):")
    print(f"   âŒ å¤±æ•—è‚¡ç¥¨: {len(failed_stocks)}")
    print(f"   â³ æœªè™•ç†è‚¡ç¥¨: {len(not_processed_stocks)}")  
    print(f"   âœ… ä»Šæ—¥æˆåŠŸ: {len(successful_today)}")
    print(f"   ğŸ•’ éæœŸæˆåŠŸ: {len(successful_old)}")
    
    if priority_stocks:
        print(f"ğŸ¯ å„ªå…ˆè™•ç†ç­–ç•¥: è™•ç† {len(priority_stocks)} å€‹å¤±æ•—/æœªè™•ç†è‚¡ç¥¨")
        return priority_stocks, "PRIORITY"
    elif successful_old and not successful_today:
        print(f"ğŸ”„ å…¨é¢æ›´æ–°ç­–ç•¥: æ‰€æœ‰è‚¡ç¥¨æˆåŠŸä½†è³‡æ–™éæœŸï¼ŒåŸ·è¡Œå®Œæ•´æƒæ")
        return all_stock_ids, "FULL_REFRESH"
    elif successful_today:
        print(f"âœ… ç„¡éœ€è™•ç†: æ‰€æœ‰è‚¡ç¥¨ä»Šæ—¥å·²æˆåŠŸè™•ç†")
        return [], "UP_TO_DATE"
    else:
        print(f"ğŸ†• åˆå§‹æƒæ: åŸ·è¡Œé¦–æ¬¡å®Œæ•´æƒæ")
        return all_stock_ids, "INITIAL_SCAN"
    """Load existing CSV data from the specific folder"""
    csv_filepath = os.path.join(folder_name, "download_results.csv")
    existing_data = {}
    
    if os.path.exists(csv_filepath):
        try:
            with open(csv_filepath, 'r', newline='', encoding='utf-8') as csvfile:
                reader = csv.DictReader(csvfile)
                for row in reader:
                    filename = row.get('filename', '')
                    if filename:
                        existing_data[filename] = {
                            'last_update_time': row.get('last_update_time', 'NEVER'),
                            'success': row.get('success', 'false'),
                            'process_time': row.get('process_time', 'NOT_PROCESSED')
                        }
            print(f"ğŸ“– å¾ {csv_filepath} è¼‰å…¥ {len(existing_data)} ç­†ç¾æœ‰è¨˜éŒ„")
        except Exception as e:
            print(f"âš ï¸ è­¦å‘Š: ç„¡æ³•è¼‰å…¥ç¾æœ‰ CSV: {e}")
    else:
        print(f"ğŸ“ æ‰¾ä¸åˆ°ç¾æœ‰ {csv_filepath} - å°‡å»ºç«‹æ–°æª”æ¡ˆ")
    
    return existing_data

def save_simple_csv_results(parameter, stock_ids, results_data, process_times, stock_mapping):
    """Save CSV in the specific folder - only current data type, always 118 rows"""
    
    # Determine folder based on data type
    if parameter == '7':
        folder = "StockBzPerformance1"
    else:
        folder_mapping = {
            '1': 'DividendDetail',
            '2': 'BasicInfo', 
            '3': 'StockDetail',
            '4': 'StockBzPerformance',
            '5': 'ShowSaleMonChart',
            '6': 'EquityDistribution'
        }
        folder = folder_mapping.get(parameter, f'DataType{parameter}')
    
    # Ensure folder exists
    if not os.path.exists(folder):
        os.makedirs(folder)
        print(f"ğŸ“ å»ºç«‹è³‡æ–™å¤¾: {folder}")
    
    csv_filepath = os.path.join(folder, "download_results.csv")
    
    # Load existing data from this folder only
    existing_data = load_existing_csv_data(folder)
    
    try:
        # Write CSV for this data type only
        with open(csv_filepath, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            
            # Write header
            writer.writerow(['filename', 'last_update_time', 'success', 'process_time'])
            
            # Write data for ALL stocks for this data type
            for stock_id in stock_ids:
                company_name = stock_mapping.get(stock_id, f'è‚¡ç¥¨{stock_id}')
                
                # Generate filename for current data type
                if parameter == '7':
                    filename = f"StockBzPerformance1_{stock_id}_{company_name}_quarter.xls"
                else:
                    filename = f"{folder}_{stock_id}_{company_name}.xls"
                
                # Check if we processed this stock in current run
                if stock_id in results_data:
                    # Current run data - get fresh info
                    file_path = os.path.join(folder, filename)
                    if os.path.exists(file_path):
                        last_update = datetime.fromtimestamp(os.path.getmtime(file_path)).strftime('%Y-%m-%d %H:%M:%S')
                    else:
                        last_update = 'NEVER'
                    
                    success = str(results_data[stock_id]).lower()
                    process_time = process_times.get(stock_id, 'NOT_PROCESSED')
                else:
                    # Not processed in current run - use existing data if available
                    if filename in existing_data:
                        existing_record = existing_data[filename]
                        last_update = existing_record['last_update_time']
                        success = existing_record['success']
                        process_time = existing_record['process_time']
                    else:
                        # No existing data - set defaults
                        last_update = 'NEVER'
                        success = 'false'
                        process_time = 'NOT_PROCESSED'
                
                # Write row
                writer.writerow([filename, last_update, success, process_time])
        
        print(f"ğŸ“Š CSV çµæœå·²å„²å­˜: {csv_filepath}")
        
        # Enhanced summary for this data type only
        if results_data:  # Only show summary if we processed stocks
            total_stocks = len(stock_ids)
            processed_count = len(results_data)
            success_count = sum(1 for success in results_data.values() if success)
            success_rate = (success_count / processed_count * 100) if processed_count > 0 else 0
            
            print(f"ğŸ“ˆ {folder} æ‘˜è¦:")
            print(f"   CSV ç¸½è‚¡ç¥¨æ•¸: {total_stocks}")
            print(f"   æœ¬æ¬¡è™•ç†è‚¡ç¥¨æ•¸: {processed_count}")
            print(f"   æœ¬æ¬¡æˆåŠŸæ•¸: {success_count}")
            print(f"   æœ¬æ¬¡æˆåŠŸç‡: {success_rate:.1f}%")
            print(f"   CSV ä½ç½®: {csv_filepath}")
        
    except Exception as e:
        print(f"âŒ å„²å­˜ CSV æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

def run_get_good_info(stock_id, parameter, debug_mode=False, direct_mode=False):
    """Run GetGoodInfo.py for a single stock with enhanced error handling"""
    try:
        cmd = ['python', 'GetGoodInfo.py', str(stock_id), str(parameter)]
        print(f"åŸ·è¡Œ: {' '.join(cmd)}")
        
        # Set environment to use UTF-8
        env = os.environ.copy()
        env['PYTHONIOENCODING'] = 'utf-8'
        
        # Adjust timeout based on data type (special workflows need more time)
        timeout = 300 if parameter in ['5', '7'] else 100  # Extra time for special workflows
        
        # Run the command
        result = subprocess.run(cmd, 
                              capture_output=True, 
                              text=True, 
                              timeout=timeout,
                              env=env,
                              encoding='utf-8',
                              errors='replace')  # Replace problematic characters
        
        if result.returncode == 0:
            print(f"[OK] {stock_id} è™•ç†æˆåŠŸ")
            if result.stdout:
                print(f"è¼¸å‡º: {result.stdout.strip()}")
        else:
            print(f"[FAIL] {stock_id} è™•ç†å¤±æ•— (é€€å‡ºç¢¼: {result.returncode})")
            
            # Show both stdout and stderr
            if result.stdout and result.stdout.strip():
                print(f"æ¨™æº–è¼¸å‡º: {result.stdout.strip()}")
            
            if result.stderr and result.stderr.strip():
                error_msg = result.stderr.strip()
                if debug_mode:
                    print(f"æ¨™æº–éŒ¯èª¤: {error_msg}")
                else:
                    error_lines = error_msg.split('\n')
                    if len(error_lines) > 3:
                        print(f"æ¨™æº–éŒ¯èª¤: {error_lines[0]}")
                        print(f"         {error_lines[1]}")
                        print(f"         ... (å…± {len(error_lines)} è¡ŒéŒ¯èª¤ï¼Œä½¿ç”¨ --debug æŸ¥çœ‹å®Œæ•´è¨Šæ¯)")
                    else:
                        print(f"æ¨™æº–éŒ¯èª¤: {error_msg}")
            
            if not result.stdout.strip() and not result.stderr.strip():
                print("éŒ¯èª¤: ç„¡ä»»ä½•è¼¸å‡ºè¨Šæ¯")
        
        return result.returncode == 0
        
    except subprocess.TimeoutExpired:
        timeout_msg = f"[TIMEOUT] {stock_id} è™•ç†è¶…æ™‚"
        if parameter in ['5', '7']:
            timeout_msg += f" (è³‡æ–™é¡å‹ {parameter} éœ€è¦ç‰¹æ®Šè™•ç†æµç¨‹ï¼Œå¯èƒ½éœ€è¦æ›´é•·æ™‚é–“)"
        print(timeout_msg)
        return False
    except Exception as e:
        print(f"[ERROR] {stock_id} åŸ·è¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return False

def show_enhanced_usage():
    """Show enhanced usage information for v1.6.0"""
    print("=" * 70)
    print("ğŸš€ Enhanced Batch Stock Data Downloader (v1.6.0)")
    print("ğŸ“Š Complete 7 Data Types with Smart Processing Priority")
    print("=" * 70)
    print()
    print("ğŸ§  SMART PROCESSING FEATURES:")
    print("   ğŸ¯ Priority: Handles failed/unprocessed stocks first")
    print("   ğŸ”„ Smart Refresh: Full scan only when data is old")
    print("   â­ï¸ Skip Recent: Avoids re-processing today's successful downloads") 
    print("   ğŸ›¡ï¸ Safe: Never lose progress on cancellation")
    print()
    print("ğŸ“‹ Usage:")
    print("   python GetAll.py <DATA_TYPE> [OPTIONS]")
    print()
    print("ğŸ”¢ Data Types (Complete Coverage):")
    for dt, desc in DATA_TYPE_DESCRIPTIONS.items():
        new_badge = " ğŸ†•" if dt in ['6', '7'] else ""
        print(f"   {dt} = {desc}{new_badge}")
    print()
    print("ğŸ”§ Options:")
    print("   --test   = Process only first 3 stocks (testing)")
    print("   --debug  = Show detailed error messages")
    print("   --direct = Simple execution mode (compatibility test)")
    print()
    print("ğŸ“Š Enhanced Examples:")
    print("   python GetAll.py 1          # Smart processing: dividend data")
    print("   python GetAll.py 4          # Smart processing: business performance")  
    print("   python GetAll.py 5          # Smart processing: monthly revenue")
    print("   python GetAll.py 6          # Smart processing: equity distribution ğŸ†•")
    print("   python GetAll.py 7          # Smart processing: quarterly performance ğŸ†•")
    print("   python GetAll.py 2 --test   # Manual: basic info (test mode)")
    print("   python GetAll.py 6 --debug  # NEW! Equity with debug output")
    print("   python GetAll.py 7 --test   # NEW! Quarterly performance (test)")
    print()
    print("ğŸ’¡ Smart Processing Notes:")
    print("   â€¢ Automatically prioritizes failed/unprocessed stocks")
    print("   â€¢ Skips recent successful downloads to save time")
    print("   â€¢ Full refresh only when all data is successful but old")
    print("   â€¢ Delete CSV file to force complete re-processing")
    print()
    print("â° GitHub Actions Automation Schedule:")
    print("   Daily 8-12 PM UTC: Types 1, 4, 5, 6, 7 (All automated)")
    print("   Manual 24/7: Types 2, 3 (On-demand data)")
    print()

def main():
    """Enhanced main function with CSV result tracking and graceful termination"""
    global current_results_data, current_process_times, current_stock_ids, current_parameter, current_stock_mapping
    
    print("=" * 70)
    print("ğŸš€ Enhanced Batch Stock Data Downloader (v1.6.0)")
    print("ğŸ“Š Complete 7 Data Types with Smart Processing Priority")
    print("ğŸ›¡ï¸ Graceful termination protection enabled")
    print("=" * 70)
    
    # Check command line arguments
    if len(sys.argv) < 2:
        show_enhanced_usage()
        print("âŒ Error: Please provide DATA_TYPE parameter")
        print("ğŸ’¡ Examples:")
        print("   python GetAll.py 1      # Dividend data")
        print("   python GetAll.py 6      # NEW! Equity distribution")
        print("   python GetAll.py 7      # NEW! Quarterly performance")
        sys.exit(1)
    
    parameter = sys.argv[1]
    test_mode = '--test' in sys.argv
    debug_mode = '--debug' in sys.argv
    direct_mode = '--direct' in sys.argv
    csv_file = "StockID_TWSE_TPEX.csv"
    
    # Validate data type
    if parameter not in DATA_TYPE_DESCRIPTIONS:
        print(f"âŒ Invalid data type: {parameter}")
        print("âœ… Valid data types:")
        for dt, desc in DATA_TYPE_DESCRIPTIONS.items():
            new_badge = " ğŸ†•" if dt in ['6', '7'] else ""
            print(f"   {dt} = {desc}{new_badge}")
        sys.exit(1)
    
    # Check if CSV file exists
    if not os.path.exists(csv_file):
        print(f"[ERROR] æ‰¾ä¸åˆ°æª”æ¡ˆ: {csv_file}")
        print("è«‹å…ˆåŸ·è¡Œ Getè§€å¯Ÿåå–®.py ä¸‹è¼‰è‚¡ç¥¨æ¸…å–®")
        print("å‘½ä»¤: python Getè§€å¯Ÿåå–®.py")
        sys.exit(1)
    
    # Check if GetGoodInfo.py exists
    if not os.path.exists("GetGoodInfo.py"):
        print("[ERROR] æ‰¾ä¸åˆ° GetGoodInfo.py")
        print("è«‹ç¢ºèª GetGoodInfo.py å­˜åœ¨æ–¼åŒä¸€ç›®éŒ„ä¸‹")
        sys.exit(1)
    
    # Read stock IDs
    print(f"[è®€å–] è®€å–è‚¡ç¥¨æ¸…å–®: {csv_file}")
    stock_ids = read_stock_ids(csv_file)
    
    if not stock_ids:
        print("[ERROR] æœªæ‰¾åˆ°æœ‰æ•ˆçš„è‚¡ç¥¨ä»£ç¢¼")
        sys.exit(1)
    
    # Load stock mapping for CSV export
    print(f"[è®€å–] è¼‰å…¥è‚¡ç¥¨åç¨±å°æ‡‰...")
    stock_mapping = load_stock_mapping(csv_file)
    
    # Set global variables for signal handler
    current_stock_ids = stock_ids
    current_parameter = parameter  
    current_stock_mapping = stock_mapping
    
    print(f"[çµ±è¨ˆ] æ‰¾åˆ° {len(stock_ids)} æ”¯è‚¡ç¥¨")
    print(f"å‰5æ”¯è‚¡ç¥¨: {stock_ids[:5]}")  # Show first 5 for verification
    
    # Get data type description
    data_desc = DATA_TYPE_DESCRIPTIONS.get(parameter, f"Data Type {parameter}")
    
    if test_mode:
        stock_ids = stock_ids[:3]  # Only process first 3 stocks in test mode
        print(f"[æ¸¬è©¦æ¨¡å¼] åªè™•ç†å‰ {len(stock_ids)} æ”¯è‚¡ç¥¨")
    
    if debug_mode:
        print("[é™¤éŒ¯æ¨¡å¼] å°‡é¡¯ç¤ºå®Œæ•´éŒ¯èª¤è¨Šæ¯")
    
    if direct_mode:
        print("[ç›´æ¥æ¨¡å¼] æ¸¬è©¦ GetGoodInfo.py æ˜¯å¦å¯æ­£å¸¸åŸ·è¡Œ")
        # Test GetGoodInfo.py directly first
        print("æ¸¬è©¦åŸ·è¡Œ: python GetGoodInfo.py")
        try:
            result = subprocess.run(['python', 'GetGoodInfo.py'], 
                                  capture_output=True, 
                                  text=True, 
                                  timeout=600)
            print(f"ç›´æ¥åŸ·è¡Œçµæœ - é€€å‡ºç¢¼: {result.returncode}")
            if result.stdout:
                print(f"æ¨™æº–è¼¸å‡º: {result.stdout}")
            if result.stderr:
                print(f"æ¨™æº–éŒ¯èª¤: {result.stderr}")
        except Exception as e:
            print(f"ç›´æ¥åŸ·è¡Œå¤±æ•—: {e}")
        print("-" * 40)
    
    print(f"ğŸ“Š è³‡æ–™é¡å‹: {data_desc}")
    print(f"åƒæ•¸: {parameter}")
    
    # Show special workflow information
    if parameter == '5':
        print("ğŸ”„ ç‰¹æ®Šæµç¨‹: æ¯æœˆç‡Ÿæ”¶ - è‡ªå‹•é»æ“Š 'æŸ¥20å¹´' æŒ‰éˆ•")
    elif parameter == '6':
        print("ğŸ“ˆ NEW! è‚¡æ±çµæ§‹åˆ†æ - æ¨™æº– XLS ä¸‹è¼‰")
    elif parameter == '7':
        print("ğŸ”„ NEW! ç‰¹æ®Šæµç¨‹: æ¯å­£ç¶“ç‡Ÿç¸¾æ•ˆ - ç‰¹æ®Š URL + è‡ªå‹•é»æ“Š 'æŸ¥60å¹´' æŒ‰éˆ•")
    
    print(f"é–‹å§‹æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("-" * 70)
    
    # SMART PROCESSING: Determine which stocks actually need processing
    print("ğŸ§  æ™ºæ…§è™•ç†åˆ†æä¸­...")
    stocks_to_process, processing_strategy = determine_stocks_to_process(parameter, stock_ids, stock_mapping)
    
    if not stocks_to_process:
        print("âœ… æ‰€æœ‰è³‡æ–™éƒ½æ˜¯æœ€æ–°çš„ï¼Œç„¡éœ€è™•ç†ï¼")
        print("ğŸ“Š ç”¢ç”Ÿ CSV ç¢ºèª...")
        save_simple_csv_results(parameter, stock_ids, {}, {}, stock_mapping)
        print("ğŸ‰ ä»»å‹™å®Œæˆï¼")
        return
    
    # Update the processing list
    original_count = len(stock_ids)
    if test_mode and processing_strategy != "UP_TO_DATE":
        stocks_to_process = stocks_to_process[:3]  # Apply test mode limit
        print(f"[æ¸¬è©¦æ¨¡å¼] é™åˆ¶è™•ç† {len(stocks_to_process)} æ”¯è‚¡ç¥¨")
    
    processing_count = len(stocks_to_process)
    print(f"ğŸ“‹ è™•ç†ç­–ç•¥: {processing_strategy}")
    print(f"ğŸ“Š è™•ç†ç¯„åœ: {processing_count}/{original_count} æ”¯è‚¡ç¥¨")
    print("-" * 70)
    
    # Process each stock with detailed tracking and incremental CSV updates
    success_count = 0
    failed_count = 0
    results_data = {}  # stock_id -> True/False
    process_times = {}  # stock_id -> process_time_string
    
    # Generate initial CSV with all stocks (preserving existing data)
    print(f"ğŸ“Š åˆå§‹åŒ– CSV æª”æ¡ˆ...")
    save_simple_csv_results(parameter, stock_ids, {}, {}, stock_mapping)
    
    # Process only the selected stocks (smart processing)
    for i, stock_id in enumerate(stocks_to_process, 1):
        print(f"\n[{i}/{len(stocks_to_process)}] è™•ç†è‚¡ç¥¨: {stock_id}")
        
        # Record process time when we start processing this stock
        current_process_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        process_times[stock_id] = current_process_time
        
        success = run_get_good_info(stock_id, parameter, debug_mode, direct_mode)
        results_data[stock_id] = success
        
        # Update global variables for signal handler
        current_results_data = results_data.copy()
        current_process_times = process_times.copy()
        
        if success:
            success_count += 1
        else:
            failed_count += 1
        
        # INCREMENTAL CSV UPDATE - Save progress after each stock
        # This ensures we don't lose progress if cancelled
        try:
            save_simple_csv_results(parameter, stock_ids, results_data, process_times, stock_mapping)
            print(f"   ğŸ“„ CSV å·²æ›´æ–° ({i}/{len(stocks_to_process)} å®Œæˆ)")
        except Exception as e:
            print(f"   âš ï¸ CSV æ›´æ–°å¤±æ•—: {e}")
        
        # Add small delay to avoid overwhelming the target system
        # Longer delay for special workflows
        delay = 2 if parameter in ['5', '7'] else 1
        if i < len(stocks_to_process):  # Don't sleep after the last item
            time.sleep(delay)
    
    # Final CSV generation (redundant but ensures completeness)
    print("\n" + "=" * 70)
    print("ğŸ“Š æœ€çµ‚ CSV çµæœ...")
    save_simple_csv_results(parameter, stock_ids, results_data, process_times, stock_mapping)
    
    # Enhanced Summary
    print("\n" + "=" * 70)
    print("ğŸ¯ Enhanced Execution Summary (v1.5.0) - Smart Processing")
    print("=" * 70)
    print(f"ğŸ“Š è³‡æ–™é¡å‹: {data_desc}")
    print(f"ğŸ“‹ è™•ç†ç­–ç•¥: {processing_strategy}")
    print(f"ç¸½è‚¡ç¥¨æ•¸: {original_count} æ”¯")
    print(f"éœ€è™•ç†è‚¡ç¥¨æ•¸: {processing_count} æ”¯") 
    print(f"å¯¦éš›è™•ç†: {len(stocks_to_process)} æ”¯è‚¡ç¥¨")
    print(f"âœ… æˆåŠŸ: {success_count} æ”¯")
    print(f"âŒ å¤±æ•—: {failed_count} æ”¯")
    if processing_count > 0:
        print(f"ğŸ“ˆ æœ¬æ¬¡æˆåŠŸç‡: {success_count/processing_count*100:.1f}%")
    print(f"çµæŸæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Show automation information
    automation_info = {
        '1': 'æ¯æ—¥è‡ªå‹•åŒ– (Daily 8 AM UTC)',
        '4': 'æ¯æ—¥è‡ªå‹•åŒ– (Daily 9 AM UTC)', 
        '5': 'æ¯æ—¥è‡ªå‹•åŒ– (Daily 10 AM UTC)',
        '6': 'æ¯æ—¥è‡ªå‹•åŒ– (Daily 11 AM UTC) ğŸ†•',
        '7': 'æ¯æ—¥è‡ªå‹•åŒ– (Daily 12 PM UTC) ğŸ†•',
        '2': 'æ‰‹å‹•åŸ·è¡Œ (Manual trigger only)',
        '3': 'æ‰‹å‹•åŸ·è¡Œ (Manual trigger only)'
    }
    
    automation = automation_info.get(parameter, 'æ‰‹å‹•åŸ·è¡Œ')
    print(f"ğŸ¤– è‡ªå‹•åŒ–ç‹€æ…‹: {automation}")
    
    # Explain the processing strategy
    strategy_explanations = {
        "PRIORITY": "ğŸ¯ å„ªå…ˆè™•ç†å¤±æ•—æˆ–æœªè™•ç†çš„è‚¡ç¥¨ï¼Œæé«˜æ•´é«”æˆåŠŸç‡",
        "FULL_REFRESH": "ğŸ”„ æ‰€æœ‰è³‡æ–™éæœŸï¼ŒåŸ·è¡Œå®Œæ•´æ›´æ–°ä»¥ç¢ºä¿è³‡æ–™æ–°é®®åº¦", 
        "UP_TO_DATE": "âœ… æ‰€æœ‰è³‡æ–™éƒ½æ˜¯æœ€æ–°çš„ï¼Œç„¡éœ€è™•ç†",
        "INITIAL_SCAN": "ğŸ†• é¦–æ¬¡æƒæï¼Œå»ºç«‹å®Œæ•´çš„è³‡æ–™åŸºç·š"
    }
    
    if processing_strategy in strategy_explanations:
        print(f"ğŸ’¡ ç­–ç•¥èªªæ˜: {strategy_explanations[processing_strategy]}")
    
    if failed_count > 0:
        print(f"\nâš ï¸ è­¦å‘Š: æœ‰ {failed_count} æ”¯è‚¡ç¥¨è™•ç†å¤±æ•—")
        print("ğŸ’¡ å»ºè­°:")
        print("   â€¢ ä½¿ç”¨ --debug æŸ¥çœ‹è©³ç´°éŒ¯èª¤è¨Šæ¯")
        print("   â€¢ ä½¿ç”¨ --test å…ˆæ¸¬è©¦å°‘æ•¸è‚¡ç¥¨")
        print("   â€¢ æª¢æŸ¥ç¶²è·¯é€£ç·šç‹€æ³")
        if parameter in ['5', '7']:
            print(f"   â€¢ è³‡æ–™é¡å‹ {parameter} ä½¿ç”¨ç‰¹æ®Šè™•ç†æµç¨‹ï¼Œå¯èƒ½éœ€è¦æ›´å¤šæ™‚é–“")
    
    if parameter in ['6', '7']:
        print(f"\nğŸ†• NEW! è³‡æ–™é¡å‹ {parameter} å·²æˆåŠŸè™•ç†!")
        print("ğŸ“ è«‹æª¢æŸ¥å°æ‡‰è³‡æ–™å¤¾ä¸­çš„æª”æ¡ˆ")

if __name__ == "__main__":
    main()